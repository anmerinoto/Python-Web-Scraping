{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping con Beautiful Soup\n",
    "\n",
    "* * *\n",
    "\n",
    "### √çconos usados ‚Äã‚Äãen este cuaderno\n",
    "üîî **Pregunta**: Una pregunta r√°pida para ayudarte a entender qu√© est√° pasando.<br>\n",
    "ü•ä **Desaf√≠o**: Ejercicio interactivo. ¬°Lo resolveremos en el taller!<br>\n",
    "‚ö†Ô∏è **Advertencia**: Atenci√≥n sobre aspectos complicados o errores comunes.<br>\n",
    "üí° **Consejo**: C√≥mo hacer algo de forma m√°s eficiente o efectiva.<br>\n",
    "üé¨ **Demostraci√≥n**: ¬°Mostrando algo m√°s avanzado para que sepas para qu√© se puede usar Python!<br>\n",
    "\n",
    "### Objetivos de aprendizaje\n",
    "1. [Reflexi√≥n: Escapar o no raspar](#when)\n",
    "2. [Extracci√≥n y an√°lisis de HTML](#extract)\n",
    "3. [Desmantelando la Asamblea General de Illinois](#scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='when'></a>\n",
    "\n",
    "# Scraping o no scraping\n",
    "\n",
    "Para acceder a datos de la web, primero debemos asegurarnos de que el sitio web que nos interesa ofrezca una API web. Plataformas como Twitter, Reddit y el New York Times ofrecen API. **Consulta el taller de D-Lab sobre [API web de Python](https://github.com/dlab-berkeley/Python-Web-APIs) si quieres aprender a usar las API.**\n",
    "\n",
    "Sin embargo, a menudo no existe una API web. En estos casos, podemos recurrir al web scraping, donde extraemos el HTML subyacente de una p√°gina web y obtenemos directamente la informaci√≥n que buscamos. Existen varios paquetes en Python que podemos usar para realizar estas tareas. Nos centraremos en dos paquetes: Requests y Beautiful Soup.\n",
    "\n",
    "Nuestro estudio de caso recopilar√° informaci√≥n sobre los senadores estatales de Illinois (http://www.ilga.gov/senate), as√≠ como la lista de proyectos de ley patrocinados por cada senador. Antes de comenzar, revise estos sitios web para conocer su estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaci√≥n\n",
    "\n",
    "Usaremos dos paquetes principales: [Requests](http://docs.python-requests.org/en/latest/user/quickstart/) y [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). Contin√∫e instalando estos paquetes, si a√∫n no lo ha hecho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# üåê La librer√≠a requests es necesaria para hacer solicitudes HTTP y descargar p√°ginas web.\n",
    "# üï∏Ô∏è Esto es fundamental para hacer web scraping (extraer informaci√≥n de p√°ginas web).\n",
    "%pip install requests  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: typing-extensions, soupsieve, beautifulsoup4\n",
      "\n",
      "   ------------- -------------------------- 1/3 [soupsieve]\n",
      "   -------------------------- ------------- 2/3 [beautifulsoup4]\n",
      "   -------------------------- ------------- 2/3 [beautifulsoup4]\n",
      "   -------------------------- ------------- 2/3 [beautifulsoup4]\n",
      "   ---------------------------------------- 3/3 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 soupsieve-2.7 typing-extensions-4.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ü•£ La instrucci√≥n %pip install beautifulsoup4 sirve para instalar la librer√≠a Beautiful Soup 4 en tu entorno de Jupyter Notebook.\n",
    "# üï∏Ô∏è Beautiful Soup es esencial para analizar y extraer informaci√≥n de archivos HTML y XML, lo que facilita el web scraping.\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n instalaremos el paquete `lxml`, que ayuda a soportar parte del an√°lisis que realiza Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\fmerino\\documents\\github\\python-web-scraping\\.venv312\\lib\\site-packages (6.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# üß© El comando %pip install lxml instala la librer√≠a lxml en tu entorno de Jupyter Notebook.\n",
    "# ‚ö° lxml es un parser r√°pido y eficiente para analizar y procesar archivos HTML y XML, muy √∫til para usar con Beautiful Soup en web scraping.\n",
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importamos las librer√≠as necesarias\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extract'></a>\n",
    "\n",
    "# Extracci√≥n y an√°lisis de HTML\n",
    "\n",
    "Para extraer y analizar correctamente HTML, seguiremos los siguientes 4 pasos:\n",
    "1. Realizar una solicitud GET\n",
    "2. Analizar la p√°gina con Beautiful Soup\n",
    "3. Buscar elementos HTML\n",
    "4. Obtener atributos y texto de estos elementos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Realiza una solicitud GET para obtener el HTML de una p√°gina\n",
    "\n",
    "Podemos usar la librer√≠a Requests para:\n",
    "\n",
    "1. Realizar una solicitud GET a la p√°gina, y\n",
    "\n",
    "2. Leer el c√≥digo HTML de la p√°gina web.\n",
    "\n",
    "El proceso de realizar una solicitud y obtener un resultado se asemeja al flujo de trabajo de una API web. Sin embargo, en este caso estamos haciendo la solicitud directamente al sitio web y tendremos que analizar el HTML por nuestra cuenta. Esto es diferente a cuando se nos proporciona la informaci√≥n ya organizada en un formato m√°s sencillo como JSON o XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head id=\"Head1\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
      "    <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\" />\n",
      "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <!-- Meta Description -->\n",
      "    <meta name=\"description\" content=\"Welcome to the official government website of the Illinois General Assembly\">\n",
      "    <meta name=\"contactName\" content=\"Legislative Information System\">\n",
      "    <meta name=\"contactOrganization\" content=\"LIS Staff Services\">\n",
      "    <meta name=\"contactStreetAddress1\" content=\"705 Stratton Office Building\">\n",
      "    <meta name=\"contactCity\" content=\"Springfield\">\n",
      "    <meta name=\"contactZipcode\" content=\"62706\">\n",
      "    <meta name=\"contactNetworkAddress\" content=\"webmaster@ilga.gov\">\n",
      "    <meta name=\"contactPhoneNumber\" content=\"217-782-3944\">\n",
      "    <meta name=\"contactFaxNumber\" content=\"217-524-6059\">\n",
      "    <meta name\n"
     ]
    }
   ],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# View some output\n",
    "print(src[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Analizar la p√°gina con Beautiful Soup\n",
    "\n",
    "Ahora, usamos la funci√≥n `BeautifulSoup` para analizar la respuesta en un √°rbol HTML. Esto devuelve un objeto (llamado **objeto soup**) que contiene todo el HTML del documento original.\n",
    "\n",
    "Si se produce un error relacionado con una biblioteca de an√°lisis, aseg√∫rese de haber instalado el paquete `lxml` para proporcionar a Beautiful Soup las herramientas de an√°lisis necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head id=\"Head1\">\n",
      "  <meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n",
      "  <meta content=\"text/html;charset=utf-8\" http-equiv=\"content-type\"/>\n",
      "  <meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <!-- Meta Description -->\n",
      "  <meta content=\"Welcome to the official government website of the Illinois General Assembly\" name=\"description\"/>\n",
      "  <meta content=\"Legislative Information System\" name=\"contactName\"/>\n",
      "  <meta content=\"LIS Staff Services\" name=\"contactOrganization\"/>\n",
      "  <meta content=\"705 Stratton Office Building\" name=\"contactStreetAddress1\"/>\n",
      "  <meta content=\"Springfield\" name=\"contactCity\"/>\n",
      "  <meta content=\"62706\" name=\"contactZipcode\"/>\n",
      "  <meta content=\"webmaster@ilga.gov\" name=\"contactNetworkAddress\"/>\n",
      "  <meta content=\"217-782-3944\" name=\"contactPhoneNumber\"/>\n",
      "  <meta content=\"217-524-6059\" name=\"contactFaxNumber\"/>\n",
      "  <meta content=\"State Of Illinois\" name=\"originatorJur\n"
     ]
    }
   ],
   "source": [
    "# Parse the response into an HTML tree\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "# Take a look\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida se ve bastante similar a la anterior, pero ahora est√° organizada en un objeto 'soup' que nos permite recorrer la p√°gina m√°s f√°cilmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Buscar elementos HTML\n",
    "\n",
    "Beautiful Soup cuenta con varias funciones para encontrar componentes √∫tiles en una p√°gina. Beautiful Soup permite encontrar elementos por:\n",
    "\n",
    "1. Etiquetas HTML\n",
    "2. Atributos HTML\n",
    "3. Selectores CSS\n",
    "\n",
    "Primero, busquemos **etiquetas HTML**.\n",
    "\n",
    "La funci√≥n `find_all` busca en el √°rbol `soup` todos los elementos con una etiqueta HTML espec√≠fica y los devuelve.\n",
    "\n",
    "¬øQu√© hace el siguiente ejemplo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"en\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-us\"></span> English\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"af\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-za\"></span> Afrikaans\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"sq\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-al\"></span> Albanian\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"ar\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-ae\"></span> Arabic\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"hy\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-am\"></span> Armenian\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"az\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-az\"></span> Azerbaijani\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"eu\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-eu\"></span> Basque\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"bn\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-bd\"></span> Bengali\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"bs\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-ba\"></span> Bosnian\n",
      "                            </a>, <a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"ca\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-es\"></span> Catalan\n",
      "                            </a>]\n"
     ]
    }
   ],
   "source": [
    "# Find all elements with a certain tag\n",
    "a_tags = soup.find_all(\"a\")\n",
    "print(a_tags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que `find_all()` es el m√©todo m√°s popular en la API de b√∫squeda de Beautiful Soup, puedes usar un atajo. Si tratas el objeto BeautifulSoup como si fuera una funci√≥n, es lo mismo que llamar a `find_all()` en ese objeto.\n",
    "\n",
    "Estas dos l√≠neas de c√≥digo son equivalentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"en\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-us\"></span> English\n",
      "                            </a>\n",
      "<a b-0yw6sxot5c=\"\" class=\"dropdown-item\" data-lang=\"en\" href=\"#\">\n",
      "<span b-0yw6sxot5c=\"\" class=\"flag-icon flag-icon-us\"></span> English\n",
      "                            </a>\n"
     ]
    }
   ],
   "source": [
    "a_tags = soup.find_all(\"a\")\n",
    "a_tags_alt = soup(\"a\")\n",
    "print(a_tags[0])\n",
    "print(a_tags_alt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øCuantos enlaces obtuvimos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "print(len(a_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Eso es much√≠simo! Muchos elementos de una p√°gina tendr√°n la misma etiqueta HTML. Por ejemplo, si buscas todo con la etiqueta `a`, probablemente obtendr√°s m√°s resultados, muchos de los cuales quiz√°s no quieras. Recuerda que la etiqueta `a` define un hiperv√≠nculo, por lo que normalmente encontrar√°s muchos en cualquier p√°gina.\n",
    "\n",
    "¬øQu√© suceder√≠a si quisi√©ramos buscar etiquetas HTML con ciertos atributos, como clases CSS espec√≠ficas?\n",
    "\n",
    "Podemos hacerlo a√±adiendo un argumento adicional a `find_all`. En el siguiente ejemplo, buscamos todas las etiquetas `a` y luego las filtramos con `class_=\"sidemenu\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get only the 'a' tags in 'sidemenu' class\n",
    "side_menus = soup(\"a\", class_=\"sidemenu\")\n",
    "side_menus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma m√°s eficiente de buscar elementos en un sitio web es mediante un selector CSS. Para ello, debemos usar un m√©todo diferente llamado `select()`. Simplemente pase una cadena a `.select()` para obtener todos los elementos con esa cadena como un selector CSS v√°lido.\n",
    "\n",
    "En el ejemplo anterior, podemos usar `\"a.sidemenu\"` como selector CSS, que devuelve todas las etiquetas `a` con la clase `sidemenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get elements with \"a.sidemenu\" CSS Selector.\n",
    "selected = soup.select(\"a.sidemenu\")\n",
    "selected[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o: Encontrar todo\n",
    "\n",
    "Usa BeautifulSoup para encontrar todos los elementos `a` con la clase `mainmenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Obtener los atributos y el texto de los elementos\n",
    "\n",
    "Una vez identificados los elementos, necesitamos la informaci√≥n de acceso de cada uno. Normalmente, esto implica dos cosas:\n",
    "\n",
    "1. Texto\n",
    "2. Atributos\n",
    "\n",
    "Obtener el texto dentro de un elemento es sencillo. Solo tenemos que usar el miembro `text` de un objeto `tag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m side_menu_links = soup.select(\u001b[33m\"\u001b[39m\u001b[33ma.sidemenu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Examine the first link\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m first_link = \u001b[43mside_menu_links\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(first_link)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# What class is this variable?\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Get all sidemenu links as a list\n",
    "side_menu_links = soup.select(\"a.sidemenu\")\n",
    "\n",
    "# Examine the first link\n",
    "first_link = side_menu_links[0]\n",
    "print(first_link)\n",
    "\n",
    "# What class is this variable?\n",
    "print('Class: ', type(first_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Es una etiqueta de Beautiful Soup! Esto significa que tiene un miembro \"texto\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(first_link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces necesitamos el valor de ciertos atributos. Esto es especialmente relevante para las etiquetas ¬´a¬ª o enlaces, donde el atributo ¬´href¬ª nos indica ad√≥nde lleva el enlace.\n",
    "\n",
    "üí° **Consejo**: Puedes acceder a los atributos de una etiqueta trat√°ndola como un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(first_link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o: Extraer atributos espec√≠ficos\n",
    "\n",
    "Extraer todos los atributos `href` de cada URL `mainmenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrape'></a>\n",
    "\n",
    "# An√°lisis de la Asamblea General de Illinois\n",
    "\n",
    "Aunque parezca incre√≠ble, estas son las herramientas fundamentales para analizar un sitio web. Una vez que dediques m√°s tiempo a familiarizarte con HTML y CSS, simplemente ser√° cuesti√≥n de comprender la estructura de un sitio web espec√≠fico y aplicar inteligentemente las herramientas de Beautiful Soup y Python.\n",
    "\n",
    "Apliquemos estas habilidades para analizar la [98.¬™ Asamblea General de Illinois](http://www.ilga.gov/senate/default.asp?GA=98).\n",
    "\n",
    "En concreto, nuestro objetivo es analizar la informaci√≥n de cada senador, incluyendo su nombre, distrito y partido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rastrear y analizar la p√°gina web\n",
    "\n",
    "Rastreemos y analicemos la p√°gina web con las herramientas que aprendimos en la secci√≥n anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# Soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscar los elementos de la tabla\n",
    "\n",
    "Nuestro objetivo es obtener los elementos de la tabla en la p√°gina web. Recuerde: las filas se identifican con la etiqueta `tr`. Usemos `find_all` para obtener estos elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all table row elements\n",
    "rows = soup.find_all(\"tr\")\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Advertencia**: Ten en cuenta que `find_all` obtiene *todos* los elementos con la etiqueta `tr`. Solo necesitamos algunos. Si usamos la funci√≥n \"Inspeccionar\" de Google Chrome y observamos con atenci√≥n, podemos usar selectores CSS para obtener solo las filas que nos interesan. En concreto, queremos las filas internas de la tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns every ‚Äòtr tr tr‚Äô css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "for row in rows[:5]:\n",
    "    print(row, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que queremos todo lo que queda despu√©s de las dos primeras filas. Empecemos con una sola fila y construyamos nuestro bucle a partir de ah√≠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row = rows[2]\n",
    "print(example_row.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desglosemos esta fila en sus celdas/columnas mediante el m√©todo `select` con selectores CSS. Si analizamos el HTML con atenci√≥n, hay un par de maneras de hacerlo.\n",
    "\n",
    "* Podr√≠amos identificar las celdas por su etiqueta `td`.\n",
    "* Podr√≠amos usar el nombre de clase `.detail`.\n",
    "* Podr√≠amos combinar ambos y usar el selector `td.detail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in example_row.select('td'):\n",
    "    print(cell)\n",
    "print()\n",
    "\n",
    "for cell in example_row.select('.detail'):\n",
    "    print(cell)\n",
    "print()\n",
    "\n",
    "for cell in example_row.select('td.detail'):\n",
    "    print(cell)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos confirmar que todos son iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert example_row.select('td') == example_row.select('.detail') == example_row.select('td.detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilicemos el selector `td.detail` para ser lo m√°s espec√≠ficos posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only those 'td' tags with class 'detail' \n",
    "detail_cells = example_row.select('td.detail')\n",
    "detail_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayor√≠a de las veces, nos interesa el **texto** real de un sitio web, no sus etiquetas. Recordemos que para obtener el texto de un elemento HTML, usamos el miembro `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the text in each of those cells\n",
    "row_data = [cell.text for cell in detail_cells]\n",
    "\n",
    "print(row_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Se ve bien! Ahora solo necesitamos usar nuestros conocimientos b√°sicos de Python para obtener los elementos de esta lista que necesitamos. Recuerda: queremos el nombre del senador, su distrito y su partido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row_data[0]) # Name\n",
    "print(row_data[3]) # District\n",
    "print(row_data[4]) # Party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando filas basura\n",
    "\n",
    "Vimos al principio que no todas las filas que obtuvimos corresponden a un senador. Tendremos que hacer limpieza antes de continuar. Vean algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Row 0:\\n', rows[0], '\\n')\n",
    "print('Row 1:\\n', rows[1], '\\n')\n",
    "print('Last Row:\\n', rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al escribir nuestro bucle for, queremos que solo se aplique a las filas relevantes. Por lo tanto, debemos filtrar las filas irrelevantes. Para ello, comparamos algunas de estas filas con las que necesitamos, observamos sus diferencias y luego formulamos esto en una condici√≥n.\n",
    "\n",
    "Como puedes imaginar, hay muchas maneras de hacerlo, y depender√° del sitio web. Aqu√≠ te mostraremos algunas para que te hagas una idea de c√≥mo hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad rows\n",
    "print(len(rows[0]))\n",
    "print(len(rows[1]))\n",
    "\n",
    "# Good rows\n",
    "print(len(rows[2]))\n",
    "print(len(rows[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz√°s las buenas filas tengan una longitud de 5. Comprob√©moslo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_rows = [row for row in rows if len(row) == 5]\n",
    "\n",
    "# Let's check some rows\n",
    "print(good_rows[0], '\\n')\n",
    "print(good_rows[-2], '\\n')\n",
    "print(good_rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos una fila de pie de p√°gina en nuestra lista que queremos evitar. Probemos algo diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[2].select('td.detail') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad row\n",
    "print(rows[-1].select('td.detail'), '\\n')\n",
    "\n",
    "# Good row\n",
    "print(rows[5].select('td.detail'), '\\n')\n",
    "\n",
    "# How about this?\n",
    "good_rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "print(\"Checking rows...\\n\")\n",
    "print(good_rows[0], '\\n')\n",
    "print(good_rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Parece que encontramos algo que funcion√≥!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unir todo en un bucle\n",
    "\n",
    "Ahora que hemos visto c√≥mo obtener los datos que queremos de una fila y filtrar las filas que no necesitamos, vamos a unirlo todo en un bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define storage list\n",
    "members = []\n",
    "\n",
    "# Get rid of junk rows\n",
    "valid_rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "# Loop through all rows\n",
    "for row in valid_rows:\n",
    "    # Select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail')\n",
    "    # Keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    # Collect information\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "    # Store in a tuple\n",
    "    senator = (name, district, party)\n",
    "    # Append to list\n",
    "    members.append(senator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be 61\n",
    "len(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a lo que tenemos en \"miembros\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(members[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä  Desaf√≠o: Obtener elementos `href` que apunten a los proyectos de ley de los miembros\n",
    "\n",
    "El c√≥digo anterior recupera informaci√≥n sobre:\n",
    "\n",
    "- el nombre del senador,\n",
    "- su n√∫mero de distrito,\n",
    "- y su partido.\n",
    "\n",
    "Ahora queremos recuperar la URL de la lista de proyectos de ley de cada senador. Cada URL seguir√° un formato espec√≠fico.\n",
    "\n",
    "El formato de la lista de proyectos de ley de un senador determinado es:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?GA=98&MemberID=[MEMBER_ID]&Primary=True`\n",
    "\n",
    "para obtener algo como:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True`\n",
    "\n",
    "en el cual `MEMBER_ID=1911`. \n",
    "\n",
    "Deber√≠as poder ver que, lamentablemente, `MEMBER_ID` no se extrae actualmente en nuestro c√≥digo de extracci√≥n.\n",
    "\n",
    "Tu tarea inicial es modificar el c√≥digo anterior para que tambi√©n **recuperemos la URL completa que apunta a la p√°gina correspondiente de los proyectos de ley patrocinados por las primarias**, para cada miembro, y la devolvamos junto con su nombre, distrito y partido.\n",
    "\n",
    "Consejos:\n",
    "\n",
    "* Para ello, deber√°s obtener el elemento de anclaje apropiado (`<a>`) en la fila de la tabla de cada legislador. Puedes usar el m√©todo `.select()` en el objeto `row` del bucle para hacerlo, similar al comando que encuentra todas las celdas `td.detail` de la fila. Recuerda que solo queremos el enlace a los proyectos de ley del legislador, no a los comit√©s ni a su p√°gina de perfil.\n",
    "* El HTML de los elementos de anclaje se ver√° como `<a href=\"/senate/Senator.asp/...\">Proyectos de ley</a>`. La cadena del atributo `href` contiene el enlace **relativo** que buscamos. Puedes acceder a un atributo de un objeto `Tag` de BeatifulSoup de la misma manera que accedes a un diccionario de Python: `anchor['attributeName']`. Consulta la <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag\">documentaci√≥n</a> para m√°s detalles.\n",
    "* Hay muchas maneras diferentes de usar BeautifulSoup. Puedes hacer lo que necesites para extraer el `href`.\n",
    "\n",
    "El c√≥digo se ha completado parcialmente. Compl√©talo donde dice `#TU C√ìDIGO AQU√ç`. Guarda la ruta en un objeto llamado `full_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# Soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "# Create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# Returns every ‚Äòtr tr tr‚Äô css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "# Get rid of junk rows\n",
    "rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "# Loop through all rows\n",
    "for row in rows:\n",
    "    # Select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail') \n",
    "    # Keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    # Collect information\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    full_path = ''\n",
    "\n",
    "    # Store in a tuple\n",
    "    senator = (name, district, party, full_path)\n",
    "    # Append to list\n",
    "    members.append(senator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test \n",
    "# members[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä  Desaf√≠o: Modulariza tu c√≥digo\n",
    "\n",
    "Convierte el c√≥digo anterior en una funci√≥n que acepte una URL, rastree la URL para encontrar sus senadores y devuelva una lista de tuplas con informaci√≥n sobre cada senador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_members(url):\n",
    "    return [___]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "url = 'http://www.ilga.gov/senate/default.asp?GA=98'\n",
    "senate_members = get_members(url)\n",
    "len(senate_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•äDesaf√≠o pr√°ctico: Escribir una funci√≥n de scraping\n",
    "\n",
    "Queremos scraping las p√°ginas web correspondientes a los proyectos de ley patrocinados por cada proyecto de ley.\n",
    "\n",
    "Escribir una funci√≥n llamada `get_bills(url)` para analizar la URL de un proyecto de ley. Esto implica:\n",
    "\n",
    "- Solicitar la URL mediante la biblioteca <a href=\"http://docs.python-requests.org/en/latest/\">`requests`</a>\n",
    "- Usar las funciones de la biblioteca `BeautifulSoup` para encontrar todos los elementos `<td>` con la clase `billlist`\n",
    "- Devolver una _lista_ de tuplas, cada una con:\n",
    "- Descripci√≥n (2.¬™ columna)\n",
    "- C√°mara (S o H) (3.¬™ columna)\n",
    "- La √∫ltima acci√≥n (4.¬™ columna)\n",
    "- La fecha de la √∫ltima acci√≥n (5.¬™ columna)\n",
    "\n",
    "Esta funci√≥n se ha completado parcialmente. Complete el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bills(url):\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src)\n",
    "    rows = soup.select('tr')\n",
    "    bills = []\n",
    "    for row in rows:\n",
    "        # YOUR CODE HERE\n",
    "       # bill_id =\n",
    "        #description =\n",
    "        #chamber =\n",
    "        #last_action =\n",
    "        #last_action_date =\n",
    "        bill = (bill_id, description, chamber, last_action, last_action_date)\n",
    "        bills.append(bill)\n",
    "    return bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your code\n",
    "# test_url = senate_members[0][3]\n",
    "# get_bills(test_url)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer todos los proyectos de ley\n",
    "\n",
    "Finalmente, cree un diccionario `bills_dict` que asigne un n√∫mero de distrito (la clave) a una lista de proyectos de ley (el valor) provenientes de ese distrito. Puede hacerlo recorriendo en bucle todos los miembros del senado en `members_dict` y llamando a `get_bills()` para cada una de las URL de sus proyectos de ley asociados.\n",
    "\n",
    "**NOTA:** Por favor, llame a la funci√≥n `time.sleep(1)` en cada iteraci√≥n del bucle para no destruir el sitio web del estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your code\n",
    "# bills_dict[52]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
