{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping con Beautiful Soup\n",
    "\n",
    "* * * \n",
    "\n",
    "### Iconos utilizados en este notebook\n",
    "üîî **Preguntas**: Una pregunta r√°pida para ayudarte a entender qu√© est√° pasando.<br>\n",
    "ü•ä **Desaf√≠o**: Ejercicio interactivo. Lo trabajaremos en el taller.!<br>\n",
    "‚ö†Ô∏è **Advertencia**: Aviso sobre cuestiones complicadas o errores comunes.<br>\n",
    "üí° **Tip**:C√≥mo hacer algo de forma un poco m√°s eficiente o efectiva.<br>\n",
    "üé¨ **Demo**: Mostrando algo m√°s avanzado: ¬°para que sepas para qu√© se puede usar Python!<br>\n",
    "\n",
    "### Objetivos de aprendizaje\n",
    "1. [Objetivos de aprendizaje](#when)\n",
    "2. [Extracci√≥n y an√°lisis de HTML](#extract)\n",
    "3. [Desmantelando la Asamblea General de Illinois](#scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='when'></a>\n",
    "\n",
    "# ‚Äú¬øHacer scraping o no hacerlo?‚Äù\n",
    "\n",
    "Cuando queremos acceder a datos de la web, primero debemos asegurarnos de que el sitio web que nos interesa ofrezca una API web. Plataformas como Twitter, Reddit y The New York Times ofrecen API. **Echa un vistazo a D-Lab [Python Web APIs](https://github.com/dlab-berkeley/Python-Web-APIs) Taller si quieres aprender a utilizar las API.**\n",
    "\n",
    "Sin embargo, a menudo no existe una API web. En estos casos, podemos recurrir al web scraping, donde extraemos el HTML subyacente de una p√°gina web y obtenemos directamente la informaci√≥n deseada. Existen varios paquetes en Python que podemos usar para realizar estas tareas. Nos centraremos en dos paquetes: Requests y Beautiful Soup.\n",
    "\n",
    "Nuestro estudio de caso consistir√° en extraer informaci√≥n sobre el [Senadores estatales de Illinois](http://www.ilga.gov/senate), as√≠ como el [lista de facturas](http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True) Cada senador ha patrocinado. Antes de empezar, revise estos sitios web para conocer su estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalaci√≥n\n",
    "\n",
    "Utilizaremos dos paquetes principales: [Solicitudes](http://docs.python-requests.org/en/latest/user/quickstart/) y [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/). Contin√∫e e instale estos paquetes, si a√∫n no lo ha hecho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jjala\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# üåê La librer√≠a requests es necesaria para hacer solicitudes HTTP y descargar p√°ginas web.\n",
    "# üï∏Ô∏è Esto es fundamental para hacer web scraping (extraer informaci√≥n de p√°ginas web).\n",
    "%pip install requests  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=4.0.0 (from beautifulsoup4)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: typing-extensions, soupsieve, beautifulsoup4\n",
      "\n",
      "   ------------- -------------------------- 1/3 [soupsieve]\n",
      "   -------------------------- ------------- 2/3 [beautifulsoup4]\n",
      "   -------------------------- ------------- 2/3 [beautifulsoup4]\n",
      "   -------------------------- ------------- 2/3 [beautifulsoup4]\n",
      "   ---------------------------------------- 3/3 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.13.4 soupsieve-2.7 typing-extensions-4.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ü•£ La instrucci√≥n %pip install beautifulsoup4 sirve para instalar la librer√≠a Beautiful Soup 4 en tu entorno de Jupyter Notebook.\n",
    "# üï∏Ô∏è Beautiful Soup es esencial para analizar y extraer informaci√≥n de archivos HTML y XML, lo que facilita el web scraping.\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambi√©n instalaremos el paquete `lxml`, que ayuda a soportar parte del an√°lisis que realiza Beautiful Soup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-6.0.1-cp313-cp313-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading lxml-6.0.1-cp313-cp313-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/4.0 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 15.9 MB/s  0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-6.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extract'></a>\n",
    "\n",
    "# Extracci√≥n y an√°lisis de HTML\n",
    "\n",
    "Para extraer y analizar HTML correctamente, seguiremos los siguientes 4 pasos:\n",
    "1. Realizar una solicitud GET\n",
    "2. Analizar la p√°gina con Beautiful Soup\n",
    "3. Buscar elementos HTML\n",
    "4. Obtener los atributos y el texto de estos elementos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Realizar una solicitud GET para obtener el HTML de una p√°gina\n",
    "\n",
    "Podemos usar la biblioteca Requests para:\n",
    "\n",
    "1. Realizar una solicitud GET a la p√°gina y\n",
    "2. Leer el c√≥digo HTML de la p√°gina web.\n",
    "\n",
    "El proceso de realizar una solicitud y obtener un resultado es similar al del flujo de trabajo de la API web. Sin embargo, ahora realizamos una solicitud directamente al sitio web y tendremos que analizar el HTML nosotros mismos. Esto contrasta con recibir datos organizados en una salida JSON o XML m√°s sencilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# View some output\n",
    "print(src[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Analizar la p√°gina con Beautiful Soup\n",
    "\n",
    "Ahora, usamos la funci√≥n `BeautifulSoup` para analizar la respuesta en un √°rbol HTML. Esto devuelve un objeto (llamado **objeto soup**) que contiene todo el HTML del documento original.\n",
    "\n",
    "Si se produce un error relacionado con una biblioteca de an√°lisis, aseg√∫rese de haber instalado el paquete `lxml` para proporcionar a Beautiful Soup las herramientas de an√°lisis necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the response into an HTML tree\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "# Take a look\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La salida se ve bastante similar a la anterior, pero ahora est√° organizada en un objeto 'soup' que nos permite recorrer la p√°gina m√°s f√°cilmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Buscar elementos HTML\n",
    "\n",
    "Beautiful Soup cuenta con varias funciones para encontrar componentes √∫tiles en una p√°gina. Beautiful Soup permite encontrar elementos por:\n",
    "\n",
    "1. Etiquetas HTML\n",
    "2. Atributos HTML\n",
    "3. Selectores CSS\n",
    "\n",
    "Primero, busquemos **etiquetas HTML**.\n",
    "\n",
    "La funci√≥n `find_all` busca en el √°rbol `soup` todos los elementos con una etiqueta HTML espec√≠fica y los devuelve.\n",
    "\n",
    "¬øQu√© hace el siguiente ejemplo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all elements with a certain tag\n",
    "a_tags = soup.find_all(\"a\")\n",
    "print(a_tags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que `find_all()` es el m√©todo m√°s popular en la API de b√∫squeda de Beautiful Soup, puedes usar un atajo. Si tratas el objeto BeautifulSoup como si fuera una funci√≥n, es lo mismo que llamar a `find_all()` en ese objeto.\n",
    "\n",
    "Estas dos l√≠neas de c√≥digo son equivalentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_tags = soup.find_all(\"a\")\n",
    "a_tags_alt = soup(\"a\")\n",
    "print(a_tags[0])\n",
    "print(a_tags_alt[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬øCu√°ntos enlaces obtuvimos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Eso es much√≠simo! Muchos elementos de una p√°gina tendr√°n la misma etiqueta HTML. Por ejemplo, si buscas todo con la etiqueta `a`, probablemente obtendr√°s m√°s resultados, muchos de los cuales quiz√°s no quieras. Recuerda que la etiqueta `a` define un hiperv√≠nculo, por lo que normalmente encontrar√°s muchos en cualquier p√°gina.\n",
    "\n",
    "¬øQu√© suceder√≠a si quisi√©ramos buscar etiquetas HTML con ciertos atributos, como clases CSS espec√≠ficas?\n",
    "\n",
    "Podemos hacerlo a√±adiendo un argumento adicional a `find_all`. En el siguiente ejemplo, buscamos todas las etiquetas `a` y luego las filtramos con `class_=\"sidemenu\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get only the 'a' tags in 'sidemenu' class\n",
    "side_menus = soup(\"a\", class_=\"sidemenu\")\n",
    "side_menus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma m√°s eficiente de buscar elementos en un sitio web es mediante un selector CSS. Para ello, debemos usar un m√©todo diferente llamado `select()`. Simplemente pase una cadena a `.select()` para obtener todos los elementos con esa cadena como un selector CSS v√°lido.\n",
    "\n",
    "En el ejemplo anterior, podemos usar `\"a.sidemenu\"` como selector CSS, que devuelve todas las etiquetas `a` con la clase `sidemenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get elements with \"a.sidemenu\" CSS Selector.\n",
    "selected = soup.select(\"a.sidemenu\")\n",
    "selected[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•äDesaf√≠o: Encontrar todo\n",
    "\n",
    "Usa BeautifulSoup para encontrar todos los elementos `a` con la clase `mainmenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 4: Obtener los atributos y el texto de los elementos\n",
    "\n",
    "Una vez identificados los elementos, necesitamos la informaci√≥n de acceso de cada uno. Normalmente, esto implica dos cosas:\n",
    "\n",
    "1. Texto\n",
    "2. Atributos\n",
    "\n",
    "Obtener el texto dentro de un elemento es sencillo. Solo tenemos que usar el miembro `text` de un objeto `tag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all sidemenu links as a list\n",
    "side_menu_links = soup.select(\"a.sidemenu\")\n",
    "\n",
    "# Examine the first link\n",
    "first_link = side_menu_links[0]\n",
    "print(first_link)\n",
    "\n",
    "# What class is this variable?\n",
    "print('Class: ', type(first_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Es una etiqueta de Beautiful Soup! Esto significa que tiene un miembro \"texto\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(first_link.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces necesitamos el valor de ciertos atributos. Esto es especialmente relevante para las etiquetas ¬´a¬ª o enlaces, donde el atributo ¬´href¬ª nos indica ad√≥nde lleva el enlace.\n",
    "\n",
    "üí° **Consejo**: Puedes acceder a los atributos de una etiqueta trat√°ndola como un diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(first_link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o: Extraer atributos espec√≠ficos\n",
    "\n",
    "Extraer todos los atributos `href` de cada URL `mainmenu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scrape'></a>\n",
    "\n",
    "# An√°lisis de la Asamblea General de Illinois\n",
    "\n",
    "Aunque parezca incre√≠ble, estas son las herramientas fundamentales para analizar un sitio web. Una vez que dediques m√°s tiempo a familiarizarte con HTML y CSS, simplemente ser√° cuesti√≥n de comprender la estructura de un sitio web espec√≠fico y aplicar inteligentemente las herramientas de Beautiful Soup y Python.\n",
    "\n",
    "Apliquemos estas habilidades para analizar la [98.¬™ Asamblea General de Illinois](http://www.ilga.gov/senate/default.asp?GA=98).\n",
    "\n",
    "En concreto, nuestro objetivo es analizar la informaci√≥n de cada senador, incluyendo su nombre, distrito y partido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rastrear y analizar la p√°gina web\n",
    "\n",
    "Rastreemos y analicemos la p√°gina web con las herramientas que aprendimos en la secci√≥n anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# Soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscar los elementos de la tabla\n",
    "\n",
    "Nuestro objetivo es obtener los elementos de la tabla en la p√°gina web. Recuerde: las filas se identifican con la etiqueta `tr`. Usemos `find_all` para obtener estos elementos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all table row elements\n",
    "rows = soup.find_all(\"tr\")\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Advertencia**: Ten en cuenta que `find_all` obtiene *todos* los elementos con la etiqueta `tr`. Solo necesitamos algunos. Si usamos la funci√≥n \"Inspeccionar\" de Google Chrome y observamos con atenci√≥n, podemos usar selectores CSS para obtener solo las filas que nos interesan. En concreto, queremos las filas internas de la tabla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns every ‚Äòtr tr tr‚Äô css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "\n",
    "for row in rows[:5]:\n",
    "    print(row, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que queremos todo lo que queda despu√©s de las dos primeras filas. Empecemos con una sola fila y construyamos nuestro bucle a partir de ah√≠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row = rows[2]\n",
    "print(example_row.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desglosemos esta fila en sus celdas/columnas mediante el m√©todo `select` con selectores CSS. Si analizamos el HTML con atenci√≥n, hay un par de maneras de hacerlo.\n",
    "\n",
    "* Podr√≠amos identificar las celdas por su etiqueta `td`.\n",
    "* Podr√≠amos usar el nombre de clase `.detail`.\n",
    "* Podr√≠amos combinar ambos y usar el selector `td.detail`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in example_row.select('td'):\n",
    "    print(cell)\n",
    "print()\n",
    "\n",
    "for cell in example_row.select('.detail'):\n",
    "    print(cell)\n",
    "print()\n",
    "\n",
    "for cell in example_row.select('td.detail'):\n",
    "    print(cell)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos confirmar que todos son iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert example_row.select('td') == example_row.select('.detail') == example_row.select('td.detail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilicemos el selector `td.detail` para ser lo m√°s espec√≠ficos posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only those 'td' tags with class 'detail' \n",
    "detail_cells = example_row.select('td.detail')\n",
    "detail_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayor√≠a de las veces, nos interesa el **texto** real de un sitio web, no sus etiquetas. Recordemos que para obtener el texto de un elemento HTML, usamos el miembro `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the text in each of those cells\n",
    "row_data = [cell.text for cell in detail_cells]\n",
    "\n",
    "print(row_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Se ve bien! Ahora solo necesitamos usar nuestros conocimientos b√°sicos de Python para obtener los elementos de esta lista que necesitamos. Recuerda: queremos el nombre del senador, su distrito y su partido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row_data[0]) # Name\n",
    "print(row_data[3]) # District\n",
    "print(row_data[4]) # Party"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminando filas basura\n",
    "\n",
    "Vimos al principio que no todas las filas que obtuvimos corresponden a un senador. Tendremos que hacer limpieza antes de continuar. Vean algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Row 0:\\n', rows[0], '\\n')\n",
    "print('Row 1:\\n', rows[1], '\\n')\n",
    "print('Last Row:\\n', rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al escribir nuestro bucle for, queremos que solo se aplique a las filas relevantes. Por lo tanto, debemos filtrar las filas irrelevantes. Para ello, comparamos algunas de estas filas con las que necesitamos, observamos sus diferencias y luego formulamos esto en una condici√≥n.\n",
    "\n",
    "Como puedes imaginar, hay muchas maneras de hacerlo, y depender√° del sitio web. Aqu√≠ te mostraremos algunas para que te hagas una idea de c√≥mo hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad rows\n",
    "print(len(rows[0]))\n",
    "print(len(rows[1]))\n",
    "\n",
    "# Good rows\n",
    "print(len(rows[2]))\n",
    "print(len(rows[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz√°s las buenas filas tengan una longitud de 5. Comprob√©moslo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_rows = [row for row in rows if len(row) == 5]\n",
    "\n",
    "# Let's check some rows\n",
    "print(good_rows[0], '\\n')\n",
    "print(good_rows[-2], '\\n')\n",
    "print(good_rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos una fila de pie de p√°gina en nuestra lista que queremos evitar. Probemos algo diferente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[2].select('td.detail') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad row\n",
    "print(rows[-1].select('td.detail'), '\\n')\n",
    "\n",
    "# Good row\n",
    "print(rows[5].select('td.detail'), '\\n')\n",
    "\n",
    "# How about this?\n",
    "good_rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "print(\"Checking rows...\\n\")\n",
    "print(good_rows[0], '\\n')\n",
    "print(good_rows[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¬°Parece que encontramos algo que funcion√≥!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unir todo en un bucle\n",
    "\n",
    "Ahora que hemos visto c√≥mo obtener los datos que queremos de una fila y filtrar las filas que no necesitamos, vamos a unirlo todo en un bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define storage list\n",
    "members = []\n",
    "\n",
    "# Get rid of junk rows\n",
    "valid_rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "# Loop through all rows\n",
    "for row in valid_rows:\n",
    "    # Select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail')\n",
    "    # Keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    # Collect information\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "    # Store in a tuple\n",
    "    senator = (name, district, party)\n",
    "    # Append to list\n",
    "    members.append(senator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be 61\n",
    "len(members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a lo que tenemos en \"miembros\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(members[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä  Desaf√≠o: Obtener elementos `href` que apunten a los proyectos de ley de los miembros\n",
    "\n",
    "El c√≥digo anterior recupera informaci√≥n sobre:\n",
    "\n",
    "- el nombre del senador,\n",
    "- su n√∫mero de distrito,\n",
    "- y su partido.\n",
    "\n",
    "Ahora queremos recuperar la URL de la lista de proyectos de ley de cada senador. Cada URL seguir√° un formato espec√≠fico.\n",
    "\n",
    "El formato de la lista de proyectos de ley de un senador determinado es:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?GA=98&MemberID=[MEMBER_ID]&Primary=True`\n",
    "\n",
    "para obtener algo como:\n",
    "\n",
    "`http://www.ilga.gov/senate/SenatorBills.asp?MemberID=1911&GA=98&Primary=True`\n",
    "\n",
    "donde `MEMBER_ID=1911`.\n",
    "\n",
    "Deber√≠as poder ver que, lamentablemente, `MEMBER_ID` no se extrae actualmente en nuestro c√≥digo de extracci√≥n.\n",
    "\n",
    "Tu tarea inicial es modificar el c√≥digo anterior para que tambi√©n **recuperemos la URL completa que apunta a la p√°gina correspondiente de los proyectos de ley patrocinados por las primarias** de cada miembro, y la devolvamos junto con su nombre, distrito y partido.\n",
    "\n",
    "Consejos:\n",
    "\n",
    "* Para ello, deber√° obtener el elemento de anclaje correspondiente (`<a>`) en la fila de cada legislador de la tabla. Puede usar el m√©todo `.select()` en el objeto `row` del bucle para hacerlo, de forma similar al comando que busca todas las celdas `td.detail` de la fila. Recuerde que solo necesitamos el enlace a los proyectos de ley del legislador, no a los comit√©s ni a su p√°gina de perfil.\n",
    "* El HTML de los elementos de anclaje se ver√° como `<a href=\"/senate/Senator.asp/...\">Proyectos de ley</a>`. La cadena del atributo `href` contiene el enlace **relativo** que buscamos. Puede acceder a un atributo de un objeto `Tag` de BeatifulSoup de la misma manera que accede a un diccionario de Python: `anchor['attributeName']`. Consulta la <a href=\"http://www.crummy.com/software/BeautifulSoup/bs4/doc/#tag\">documentaci√≥n</a> para obtener m√°s detalles.\n",
    "* Hay muchas maneras diferentes de usar BeautifulSoup. Puedes usar cualquier m√©todo para extraer el `href`.\n",
    "\n",
    "El c√≥digo se ha completado parcialmente. Compl√©talo donde dice `#TU C√ìDIGO AQU√ç`. Guarda la ruta en un objeto llamado `full_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a GET request\n",
    "req = requests.get('http://www.ilga.gov/senate/default.asp?GA=98')\n",
    "# Read the content of the server‚Äôs response\n",
    "src = req.text\n",
    "# Soup it\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "# Create empty list to store our data\n",
    "members = []\n",
    "\n",
    "# Returns every ‚Äòtr tr tr‚Äô css selector in the page\n",
    "rows = soup.select('tr tr tr')\n",
    "# Get rid of junk rows\n",
    "rows = [row for row in rows if row.select('td.detail')]\n",
    "\n",
    "# Loop through all rows\n",
    "for row in rows:\n",
    "    # Select only those 'td' tags with class 'detail'\n",
    "    detail_cells = row.select('td.detail') \n",
    "    # Keep only the text in each of those cells\n",
    "    row_data = [cell.text for cell in detail_cells]\n",
    "    # Collect information\n",
    "    name = row_data[0]\n",
    "    district = int(row_data[3])\n",
    "    party = row_data[4]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    full_path = ''\n",
    "\n",
    "    # Store in a tuple\n",
    "    senator = (name, district, party, full_path)\n",
    "    # Append to list\n",
    "    members.append(senator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test \n",
    "# members[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä  Desaf√≠o: Modulariza tu c√≥digo\n",
    "\n",
    "Convierte el c√≥digo anterior en una funci√≥n que acepte una URL, rastree la URL para encontrar sus senadores y devuelva una lista de tuplas con informaci√≥n sobre cada senador. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_members(url):\n",
    "    return [___]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your code\n",
    "url = 'http://www.ilga.gov/senate/default.asp?GA=98'\n",
    "senate_members = get_members(url)\n",
    "len(senate_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o pr√°ctico: Escribir una funci√≥n de scraping\n",
    "\n",
    "Queremos scraping las p√°ginas web correspondientes a los proyectos de ley patrocinados por cada proyecto de ley.\n",
    "\n",
    "Escribir una funci√≥n llamada `get_bills(url)` para analizar la URL de un proyecto de ley. Esto implica:\n",
    "\n",
    "- Solicitar la URL mediante la biblioteca <a href=\"http://docs.python-requests.org/en/latest/\">`requests`</a>\n",
    "- Usar las funciones de la biblioteca `BeautifulSoup` para encontrar todos los elementos `<td>` con la clase `billlist`\n",
    "- Devolver una _lista_ de tuplas, cada una con:\n",
    "- Descripci√≥n (2.¬™ columna)\n",
    "- C√°mara (S o H) (3.¬™ columna)\n",
    "- La √∫ltima acci√≥n (4.¬™ columna)\n",
    "- La fecha de la √∫ltima acci√≥n (5.¬™ columna)\n",
    "\n",
    "Esta funci√≥n se ha completado parcialmente. Complete el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bills(url):\n",
    "    src = requests.get(url).text\n",
    "    soup = BeautifulSoup(src)\n",
    "    rows = soup.select('tr')\n",
    "    bills = []\n",
    "    for row in rows:\n",
    "        # YOUR CODE HERE\n",
    "        #bill_id =\n",
    "        #description =\n",
    "        #chamber =\n",
    "        #last_action =\n",
    "        #last_action_date =\n",
    "        bill = (bill_id, description, chamber, last_action, last_action_date)\n",
    "        bills.append(bill)\n",
    "    return bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your code\n",
    "# test_url = senate_members[0][3]\n",
    "# get_bills(test_url)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer todos los proyectos de ley\n",
    "\n",
    "Finalmente, cree un diccionario `bills_dict` que asigne un n√∫mero de distrito (la clave) a una lista de proyectos de ley (el valor) provenientes de ese distrito. Puede hacerlo recorriendo en bucle todos los miembros del senado en `members_dict` y llamando a `get_bills()` para cada una de las URL de sus proyectos de ley asociados.\n",
    "\n",
    "**NOTA:** Por favor, llame a la funci√≥n `time.sleep(1)` en cada iteraci√≥n del bucle para no destruir el sitio web del estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to test your code\n",
    "# bills_dict[52]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
